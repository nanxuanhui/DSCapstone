{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import numpy as np\n",
    "from transformers import pipeline, Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort\n",
    "import streamlit as st\n",
    "import faiss\n",
    "import langchain\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using YOLOv5 detect.py script with model best.pt...\n"
     ]
    }
   ],
   "source": [
    "# Load custom YOLOv5 model\n",
    "yolo_model_path = \"best.pt\"\n",
    "yolo_detect_script = \"yolov5/detect.py\"\n",
    "print(f\"Using YOLOv5 detect.py script with model {yolo_model_path}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directories exist\n",
    "uploads_dir = Path(\"uploads\")\n",
    "outputs_dir = Path(\"outputs\")\n",
    "uploads_dir.mkdir(exist_ok=True)\n",
    "outputs_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fall(image_path):\n",
    "    output_image_path = outputs_dir / Path(image_path).name\n",
    "    command = [\n",
    "        \"python\", yolo_detect_script,\n",
    "        \"--weights\", yolo_model_path,\n",
    "        \"--source\", image_path,\n",
    "        \"--save-txt\",\n",
    "        \"--project\", \"outputs\",\n",
    "        \"--name\", \"fall-detection\",\n",
    "        \"--exist-ok\"\n",
    "    ]\n",
    "    print(f\"Running YOLOv5 detection: {' '.join(command)}\")\n",
    "    subprocess.run(command, check=True)\n",
    "    processed_image_path = outputs_dir / \"fall-detection\" / Path(image_path).name\n",
    "    return str(processed_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Speech Emotion Detection model: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech Emotion Detection model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Speech Emotion Detection model\n",
    "emotion_model_name = \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n",
    "print(f\"Loading Speech Emotion Detection model: {emotion_model_name}...\")\n",
    "emotion_model = Wav2Vec2ForSequenceClassification.from_pretrained(emotion_model_name).to(dtype=torch.float32)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(emotion_model_name)\n",
    "print(\"Speech Emotion Detection model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion labels mapping\n",
    "emotions = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust', 'surprise', 'calm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_path):\n",
    "    print(f\"Processing audio file for emotion detection: {audio_path}\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample audio if needed\n",
    "    target_sample_rate = 16000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = target_sample_rate\n",
    "    \n",
    "    # Ensure correct waveform shape and dtype\n",
    "    waveform = waveform.squeeze(0).to(dtype=torch.float32)  # Remove batch dimension and ensure float32\n",
    "    inputs = feature_extractor(waveform, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_emotion = emotions[predicted_class] if predicted_class < len(emotions) else \"Unknown\"\n",
    "    print(f\"Emotion prediction complete! Predicted class: {predicted_class} ({predicted_emotion})\")\n",
    "    return predicted_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying model optimizations...\n"
     ]
    }
   ],
   "source": [
    "# Model optimization: Quantization & Pruning\n",
    "print(\"Applying model optimizations...\")\n",
    "def quantize_yolo():\n",
    "    print(\"Quantizing YOLOv5 model using ONNX...\")\n",
    "    dummy_input = torch.randn(1, 3, 640, 640)\n",
    "    torch.onnx.export(torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_model_path), dummy_input, \"yolo_quantized.onnx\", opset_version=11)\n",
    "    print(\"YOLOv5 quantization completed!\")\n",
    "\n",
    "def quantize_wav2vec():\n",
    "    print(\"Quantizing Wav2Vec2 model...\")\n",
    "    emotion_model.to(dtype=torch.float32)  # Ensure model is in float32 to avoid dtype mismatch\n",
    "    print(\"Wav2Vec2 quantization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing YOLOv5 model using ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2025-3-5 Python-3.9.21 torch-2.6.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 322 layers, 86180143 parameters, 0 gradients, 203.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:867: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:688: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:101: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 quantization completed!\n",
      "Quantizing Wav2Vec2 model...\n",
      "Wav2Vec2 quantization completed!\n"
     ]
    }
   ],
   "source": [
    "quantize_yolo()\n",
    "quantize_wav2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.31.99.212:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "Using YOLOv5 detect.py script with model best.pt...\n",
      "Loading Speech Emotion Detection model: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition...\n",
      "/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/transformers/configuration_utils.py:315: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Speech Emotion Detection model loaded successfully!\n",
      "Applying model optimizations...\n",
      "Quantizing YOLOv5 model using ONNX...\n",
      "Using cache found in /Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2025-3-5 Python-3.9.21 torch-2.6.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 322 layers, 86180143 parameters, 0 gradients, 203.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:867: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:688: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:101: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
      "YOLOv5 quantization completed!\n",
      "Quantizing Wav2Vec2 model...\n",
      "Wav2Vec2 quantization completed!\n",
      "2025-03-05 22:13:12.250 Examining the path of torch.classes raised:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
      "    potential_paths = extract_paths(module)\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
      "    lambda m: list(m.__path__._path),\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/torch/_classes.py\", line 13, in __getattr__\n",
      "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
      "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
      "Using YOLOv5 detect.py script with model best.pt...\n",
      "Loading Speech Emotion Detection model: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition...\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Speech Emotion Detection model loaded successfully!\n",
      "Applying model optimizations...\n",
      "Quantizing YOLOv5 model using ONNX...\n",
      "Using cache found in /Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2025-3-5 Python-3.9.21 torch-2.6.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 322 layers, 86180143 parameters, 0 gradients, 203.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:867: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:688: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:101: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
      "YOLOv5 quantization completed!\n",
      "Quantizing Wav2Vec2 model...\n",
      "Wav2Vec2 quantization completed!\n",
      "Received image file: 1.jpeg\n",
      "Running YOLOv5 detection: python yolov5/detect.py --weights best.pt --source uploads/1.jpeg --save-txt --project outputs --name fall-detection --exist-ok\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=uploads/1.jpeg, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=outputs, name=fall-detection, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ðŸš€ v7.0-398-g5cdad892 Python-3.9.21 torch-2.6.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 322 layers, 86180143 parameters, 0 gradients, 203.8 GFLOPs\n",
      "image 1/1 /Users/nanxuan/Desktop/2025Spring/5588/Week7/uploads/1.jpeg: 640x480 1 fallls, 147.1ms\n",
      "Speed: 0.5ms pre-process, 147.1ms inference, 0.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1moutputs/fall-detection\u001b[0m\n",
      "1 labels saved to outputs/fall-detection/labels\n",
      "2025-03-05 22:13:22.173 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
      "2025-03-05 22:13:22.452 Examining the path of torch.classes raised:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
      "    potential_paths = extract_paths(module)\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
      "    lambda m: list(m.__path__._path),\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/torch/_classes.py\", line 13, in __getattr__\n",
      "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
      "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
      "Using YOLOv5 detect.py script with model best.pt...\n",
      "Loading Speech Emotion Detection model: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition...\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Speech Emotion Detection model loaded successfully!\n",
      "Applying model optimizations...\n",
      "Quantizing YOLOv5 model using ONNX...\n",
      "Using cache found in /Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2025-3-5 Python-3.9.21 torch-2.6.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 322 layers, 86180143 parameters, 0 gradients, 203.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:867: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:688: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
      "/Users/nanxuan/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:101: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
      "YOLOv5 quantization completed!\n",
      "Quantizing Wav2Vec2 model...\n",
      "Wav2Vec2 quantization completed!\n",
      "Received image file: 1.jpeg\n",
      "Running YOLOv5 detection: python yolov5/detect.py --weights best.pt --source uploads/1.jpeg --save-txt --project outputs --name fall-detection --exist-ok\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['best.pt'], source=uploads/1.jpeg, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=outputs, name=fall-detection, exist_ok=True, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ðŸš€ v7.0-398-g5cdad892 Python-3.9.21 torch-2.6.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 322 layers, 86180143 parameters, 0 gradients, 203.8 GFLOPs\n",
      "image 1/1 /Users/nanxuan/Desktop/2025Spring/5588/Week7/uploads/1.jpeg: 640x480 1 fallls, 145.3ms\n",
      "Speed: 0.4ms pre-process, 145.3ms inference, 0.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1moutputs/fall-detection\u001b[0m\n",
      "1 labels saved to outputs/fall-detection/labels\n",
      "2025-03-05 22:13:35.049 The `use_column_width` parameter has been deprecated and will be removed in a future release. Please utilize the `use_container_width` parameter instead.\n",
      "Received audio file: DC_a01.wav\n",
      "Processing audio file for emotion detection: uploads/DC_a01.wav\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "Emotion prediction complete! Predicted class: 7 (calm)\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "Exception ignored in: <module 'threading' from '/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1447, in _shutdown\n",
      "    atexit_call()\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/concurrent/futures/thread.py\", line 31, in _python_exit\n",
      "    t.join()\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/web/bootstrap.py\", line 44, in signal_handler\n",
      "    server.stop()\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/web/server/server.py\", line 470, in stop\n",
      "    self._runtime.stop()\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/runtime/runtime.py\", line 337, in stop\n",
      "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/asyncio/base_events.py\", line 796, in call_soon_threadsafe\n",
      "    self._check_closed()\n",
      "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/asyncio/base_events.py\", line 515, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscapstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
